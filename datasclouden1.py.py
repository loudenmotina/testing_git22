# -*- coding: utf-8 -*-
"""Linear_Regression_Housing_Data (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QoaP3qSvlDwP3Li0Ppl-28WHtHNTnuor
"""

# Commented out IPython magic to ensure Python compatibility.
## Import some libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn import preprocessing

### Read the house data and try to transform into a datafram if not
df=pd.read_csv('kc_house_data.csv')
df.head(4)

### Describe the dataset
df.describe().round(1)

### id and date are irrelevant so we drop them
df=df.drop(['id','date'],axis=1)
df.head(4)

## display the number of data observations
len(df)

len(df.columns)

df.dtypes

## check Nan, missing values among the faetures, dataset
df.isnull().values.sum()

df.head(3)

### Feature Selection
target=df.iloc[:,0].name
features=df.iloc[:,1:].columns.tolist()
features

### Correlations of features with the target variable
corr=df.corr()
corr['price']

### correlated with target variable and remove/display it for a while

cor_target=abs(corr['price'])
### Display the correlated value < 0.2
removed_features=cor_target[cor_target<0.2]
removed_features

### remove the features selected above
df1=df.drop(['sqft_lot','condition','yr_built','yr_renovated','zipcode','long','sqft_lot15'],axis=1)

df1

### Plot Pearson Correlation Matrix
fig=plt.figure(figsize=(15,10))
new_corr=df1.corr()
sns.heatmap(new_corr, annot=True, cmap='Greens',annot_kws={'size':8})
plt.title('Pearson Correaltion Matrix')
plt.show()

## Determine the highest intercorrelations
highly_corr_features=new_corr[new_corr>0.75]
highly_corr_features.fillna('-')

### remove highly correlated
df2=df1.drop(['sqft_above','sqft_living15'],axis=1)

df2.head(3)

# creating feature variables
X = df2.drop('price',axis= 1)
y = df2['price']

X

y





"""MOdel Creation Starts here...Spliting the data and model chosen"""

# creating train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.35, random_state=1)

# creating a regression model
model = LinearRegression()

# fitting the model
model.fit(X_train,y_train)

# making predictions
predictions1 = model.predict(X_train)

# making predictions
predictions = model.predict(X_test)

# model evaluation
print(
  'mean_squared_error : ', mean_squared_error(y_test, predictions))
print(
  'mean_absolute_error : ', mean_absolute_error(y_test, predictions))

from sklearn.metrics import r2_score

r2_score(y_train, predictions1, force_finite=False)

r2_score(y_test, predictions, force_finite=False)

from sklearn.neighbors import KNeighborsRegressor

#from sklearn.svm import SVR
#from sklearn.neighbors import KNeighborsRegressor
#from sklearn.tree import DecisionTreeRegressor
#from sklearn.ensemble import RandomForestRegressor

# creating feature variables
X = df2.drop('price',axis= 1)
y = df2['price']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize regression models
#svm_reg = SVR()
knn_reg = KNeighborsRegressor(n_neighbors=5)
#tree_reg = DecisionTreeRegressor()
#rf_reg = RandomForestRegressor(n_estimators=100)

# Train regression models
#svm_reg.fit(X_train, y_train)
knn_reg.fit(X_train, y_train)
#tree_reg.fit(X_train, y_train)
#rf_reg.fit(X_train, y_train)

# Make predictions
#svm_pred = svm_reg.predict(X_test)
knn_pred = knn_reg.predict(X_test)
#tree_pred = tree_reg.predict(X_test)
#rf_pred = rf_reg.predict(X_test)

# Calculate mean squared errors
#svm_mse = mean_squared_error(y_test, svm_pred)
knn_mse = mean_squared_error(y_test, knn_pred)
#tree_mse = mean_squared_error(y_test, tree_pred)
#rf_mse = mean_squared_error(y_test, rf_pred)

# Create a bar plot
#algorithms = ['SVM', 'KNN', 'Decision Tree', 'Random Forest']
#mse_scores = [svm_mse, knn_mse, tree_mse, rf_mse]
r2_score(y_test, knn_pred, force_finite=False)
#plt.bar(algorithms, mse_scores)
#plt.xlabel('Regression Algorithms')
#plt.ylabel('Mean Squared Error')
#plt.title('Comparison of Regression Algorithm Performance')
#plt.ylim(0, max(mse_scores) * 1.2)  # Set y-axis limits for better visualization
#plt.show()



## SVM Regressor

from sklearn.svm import SVR


# creating feature variables
X = df2.drop('price',axis= 1)
y = df2['price']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize regression models
svm_reg = SVR()


# Train regression models
svm_reg.fit(X_train, y_train)

# Make predictions
svm_pred = svm_reg.predict(X_test)

# Calculate mean squared errors - Metrics
svm_mse = mean_squared_error(y_test, svm_pred)



r2_score(y_test, svm_pred, force_finite=False)

"""Decision Tree"""

from sklearn.tree import DecisionTreeRegressor

# creating feature variables
X = df2.drop('price',axis= 1)
y = df2['price']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize regression models

tree_reg = DecisionTreeRegressor()


# Train regression models

tree_reg.fit(X_train, y_train)


# Make predictions

tree_pred = tree_reg.predict(X_test)


# Calculate mean squared errors
tree_mse = mean_squared_error(y_test, tree_pred)


# Create a bar plot
#algorithms = ['SVM', 'KNN', 'Decision Tree', 'Random Forest']
#mse_scores = [svm_mse, knn_mse, tree_mse, rf_mse]
r2_score(y_test, tree_pred) ### Force_finite=False - grid search =
#plt.bar(algorithms, mse_scores)
#plt.xlabel('Regression Algorithms')
#plt.ylabel('Mean Squared Error')
#plt.title('Comparison of Regression Algorithm Performance')
#plt.ylim(0, max(mse_scores) * 1.2)  # Set y-axis limits for better visualization
#plt.show()

tree_pred1 = tree_reg.predict(X_train)
r2_score(y_train, tree_pred1, force_finite=False)

"""Random Forest"""

from sklearn.ensemble import RandomForestRegressor

# creating feature variables
X = df2.drop('price',axis= 1)
y = df2['price']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize regression models

rf_reg = RandomForestRegressor(n_estimators=100)

# Train regression models

rf_reg.fit(X_train, y_train)

# Make predictions

rf_pred = rf_reg.predict(X_test)

# Calculate mean squared errors
rf_mse = mean_squared_error(y_test, rf_pred)

# Create a bar plot
#algorithms = ['SVM', 'KNN', 'Decision Tree', 'Random Forest']
#mse_scores = [svm_mse, knn_mse, tree_mse, rf_mse]
r2_score(y_test, rf_pred, force_finite=False)
#plt.bar(algorithms, mse_scores)
#plt.xlabel('Regression Algorithms')
#plt.ylabel('Mean Squared Error')
#plt.title('Comparison of Regression Algorithm Performance')
#plt.ylim(0, max(mse_scores) * 1.2)  # Set y-axis limits for better visualization
#plt.show()

rf_pred1 = rf_reg.predict(X_train)

# Calculate mean squared errors
rf_mse1 = mean_squared_error(y_train, rf_pred1)

# Create a bar plot
#algorithms = ['SVM', 'KNN', 'Decision Tree', 'Random Forest']
#mse_scores = [svm_mse, knn_mse, tree_mse, rf_mse]
r2_score(y_train, rf_pred1, force_finite=False)

"""All Models combined"""

from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor

# creating feature variables
X = df2.drop('price',axis= 1)
y = df2['price']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize regression models
svm_reg = SVR()
knn_reg = KNeighborsRegressor(n_neighbors=5)
tree_reg = DecisionTreeRegressor()
rf_reg = RandomForestRegressor(n_estimators=100)
xgb_reg = GradientBoostingRegressor(n_estimators=300, learning_rate=0.3, max_depth=10, random_state=0)

# Train regression models
svm_reg.fit(X_train, y_train)
knn_reg.fit(X_train, y_train)
tree_reg.fit(X_train, y_train)
rf_reg.fit(X_train, y_train)
xgb_reg.fit(X_train, y_train)

# Make predictions
svm_pred = svm_reg.predict(X_test)
knn_pred = knn_reg.predict(X_test)
tree_pred = tree_reg.predict(X_test)
rf_pred = rf_reg.predict(X_test)
xgb_pred = xgb_reg.predict(X_test)

# Calculate mean squared errors
svm_mse = mean_squared_error(y_test, svm_pred)
knn_mse = mean_squared_error(y_test, knn_pred)
tree_mse = mean_squared_error(y_test, tree_pred)
rf_mse = mean_squared_error(y_test, rf_pred)
xgb_mse = mean_squared_error(y_test, xgb_pred)

# Create a bar plot
algorithms = ['SVM', 'KNN', 'Decision Tree', 'Random Forest', 'XGBoost']
mse_scores = [svm_mse, knn_mse, tree_mse, rf_mse,xgb_mse]

plt.bar(algorithms, mse_scores)
plt.xlabel('Regression Algorithms')
plt.ylabel('Mean Squared Error')
plt.title('Comparison of Regression Algorithm Performance')
plt.ylim(0, max(mse_scores) * 1.2)  # Set y-axis limits for better visualization
plt.show()

# Calculate R-squared scores
svm_r2 = r2_score(y_test, svm_pred)
knn_r2 = r2_score(y_test, knn_pred)
tree_r2 = r2_score(y_test, tree_pred)
rf_r2 = r2_score(y_test, rf_pred)
xgb_r2 = r2_score(y_test, xgb_pred)

# Create a bar plot
algorithms = ['SVM', 'KNN', 'Decision Tree', 'Random Forest','XGBoost']
r2_scores = [svm_r2, knn_r2, tree_r2, rf_r2,xgb_r2]

plt.bar(algorithms, r2_scores)
plt.xlabel('Regression Algorithms')
plt.ylabel('R-squared Score')
plt.title('Comparison of Regression Algorithm Performance')
plt.ylim(0.5, 1.0)  # Set y-axis limits for better visualization
plt.show()



from sklearn.ensemble import GradientBoostingRegressor